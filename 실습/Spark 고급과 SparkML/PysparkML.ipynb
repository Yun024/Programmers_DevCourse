{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Titanic Classification via ML Pipeline and Model Selection\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"data/titanic.csv\", header=True, inferSchema=True)\n",
    "data.printSchema()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select(['*']).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**데이터 클린업**\n",
    "\n",
    "*   PassengerID, Name, Ticket, Embarked는 사용하지 않을 예정 (아무 의미가 없음).\n",
    "*   Cabin도 비어있는 값이 너무 많아서 사용하지 않을 예정\n",
    "*   Age는 중요한 정보인데 비어있는 레코드들이 많아서 디폴트값을 채워줄 예정\n",
    "*   모든 필드를 MinMaxScaler로 스케일\n",
    "*   Gender의 경우 카테고리 정보이기에 숫자로 인코딩 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = data.select(['Survived', 'Pclass', 'Gender', 'Age', 'SibSp', 'Parch', 'Fare'])\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age는 평균값으로 채운다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(strategy='mean', inputCols=['Age'], outputCols=['AgeImputed'])\n",
    "imputer_model = imputer.fit(final_data)\n",
    "final_data = imputer_model.transform(final_data)\n",
    "\n",
    "final_data.select(\"Age\", \"AgeImputed\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성별 정보 인코딩: male -> 0, female -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "gender_indexer = StringIndexer(inputCol='Gender', outputCol='GenderIndexed')\n",
    "gender_indexer_model = gender_indexer.fit(final_data)\n",
    "final_data = gender_indexer_model.transform(final_data)\n",
    "\n",
    "final_data.select(\"Gender\", \"GenderIndexed\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 피쳐 벡터를 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['Pclass', 'SibSp', 'Parch', 'Fare', 'AgeImputed', 'GenderIndexed'], outputCol='features')\n",
    "data_vec = assembler.transform(final_data)\n",
    "\n",
    "data_vec.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age와 Fare의 값을 스케일하는 것이 주요 목표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "age_scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "age_scaler_model = age_scaler.fit(data_vec)\n",
    "data_vec = age_scaler_model.transform(data_vec)\n",
    "\n",
    "data_vec.select(\"features\", \"features_scaled\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련용과 테스트용 데이터를 나누고 binary classification 모델을 하나 만든다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data_vec.randomSplit([0.7, 0.3])\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "algo = LogisticRegression(featuresCol=\"features_scaled\", labelCol=\"Survived\")\n",
    "model = algo.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 성능 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)\n",
    "predictions.groupby(['Survived']).count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupby(['prediction']).count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(['Survived','prediction', 'probability']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='Survived', metricName='areaUnderROC')\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.plot(model.summary.roc.select('FPR').collect(),\n",
    "         model.summary.roc.select('TPR').collect())\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer, StringIndexer, VectorAssembler, MinMaxScaler\n",
    "\n",
    "# Gender\n",
    "stringIndexer = StringIndexer(inputCol= \"Gender\", outputCol = \"GenderIndexed\")\n",
    "\n",
    "# Age\n",
    "imputer = Imputer(strategy='mean', inputCols=['Age'], outputCols=['AgeImputed'])\n",
    "\n",
    "# Vectorize\n",
    "inputCols = ['Pclass', 'SibSp', 'Parch', 'Fare', 'AgeImputed', 'GenderIndexed']\n",
    "assembler = VectorAssembler(inputCols=inputCols, outputCol='features')\n",
    "\n",
    "# MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler(inputCol='features', outputCol='features_scaled')\n",
    "\n",
    "stages = [stringIndexer, imputer, assembler, minmax_scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql.functions import avg, col, when, isnan\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class GenderMeanImputer(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self, gender_col=\"Gender\", target_col=\"Age\", output_col=\"AgeImputed\"):\n",
    "        super(GenderMeanImputer, self).__init__()\n",
    "        self.gender_col = gender_col\n",
    "        self.target_col = target_col\n",
    "        self.output_col = output_col\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        # 성별별 평균 나이 계산\n",
    "        gender_avg = df.groupBy(self.gender_col).agg(avg(self.target_col).alias(\"AvgAge\"))\n",
    "\n",
    "        # 원본 데이터와 조인\n",
    "        df = df.join(gender_avg, on=self.gender_col, how=\"left\")\n",
    "\n",
    "        # NULL 또는 NaN 값인 경우 성별 평균으로 대체\n",
    "        df = df.withColumn(\n",
    "            self.output_col,\n",
    "            when(col(self.target_col).isNull() | isnan(col(self.target_col)), col(\"AvgAge\"))\n",
    "            .otherwise(col(self.target_col))\n",
    "        )\n",
    "\n",
    "        # 필요 없는 컬럼 삭제\n",
    "        df = df.drop(\"AvgAge\")\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "\n",
    "# Gender Indexer\n",
    "stringIndexer = StringIndexer(inputCol=\"Gender\", outputCol=\"GenderIndexed\")\n",
    "\n",
    "# Custom GenderMeanImputer 적용\n",
    "gender_mean_imputer = GenderMeanImputer(gender_col=\"Gender\", target_col=\"Age\", output_col=\"AgeImputed\")\n",
    "\n",
    "# Vectorize\n",
    "inputCols = ['Pclass', 'SibSp', 'Parch', 'Fare', 'AgeImputed', 'GenderIndexed']\n",
    "assembler = VectorAssembler(inputCols=inputCols, outputCol='features')\n",
    "\n",
    "# MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler(inputCol='features', outputCol='features_scaled')\n",
    "\n",
    "# Pipeline 생성\n",
    "stages = [stringIndexer, gender_mean_imputer, assembler, minmax_scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "algo = LogisticRegression(featuresCol='features_scaled', labelCol='Survived')\n",
    "lr_stages = stages + [algo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = lr_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.select(['Survived', 'Pclass', 'Gender', 'Age', 'SibSp', 'Parch', 'Fare'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = pipeline.fit(train)\n",
    "lr_cv_predictions = lr_model.transform(test)\n",
    "evaluator.evaluate(lr_cv_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='Survived', metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(algo.maxIter, [1, 5, 10])\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross validations\n",
    "cvModel = cv.fit(train)\n",
    "lr_cv_predictions = cvModel.transform(test)\n",
    "evaluator.evaluate(lr_cv_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv_predictions.select(\"prediction\", \"survived\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "params = [{p.name: v for p,v in m.items()} for m in cvModel.getEstimatorParamMaps()]\n",
    "pd.DataFrame.from_dict([\n",
    "    {cvModel.getEvaluator().getMetricName(): metric, **ps}\n",
    "    for ps, metric in zip(params, cvModel.avgMetrics)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(featuresCol='features_scaled', labelCol='Survived')\n",
    "gbt_stages = stages+ [gbt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages= gbt_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [2, 4, 6])\n",
    "             .addGrid(gbt.maxBins, [20, 60])\n",
    "             .addGrid(gbt.maxIter, [10, 20])\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator= pipeline,\n",
    "    estimatorParamMaps= paramGrid,\n",
    "    evaluator= evaluator,\n",
    "    numFolds= 5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross validations.\n",
    "cvModel = cv.fit(train)\n",
    "lr_cv_predictions = cvModel.transform(test)\n",
    "evaluator.evaluate(lr_cv_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

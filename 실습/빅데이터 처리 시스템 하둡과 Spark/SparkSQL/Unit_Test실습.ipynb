{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Python Spark Unit\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.driver.memory\",\"2g\") \\\n",
    "    .config(\"spark.executor.memory\",\"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'C:\\\\Users\\\\Yeojun\\\\Documents\\\\GitHub\\\\Programmers_DevCourse\\\\학습내용\\\\빅데이터 처리 시스템, 하둡과 Spark\\\\data'\n",
    "df = spark.read.option(\"header\",True).csv(path + '\\\\name_gender.csv')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     F|   65|\n",
      "|     M|   28|\n",
      "|Unisex|    7|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"namegender\")\n",
    "spark.sql(\"SELECT gender, COUNT(1) count FROM namegender GROUP BY 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upper_udf_f UDF를 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def upper_udf_f(s: pd.Series) -> pd.Series:\n",
    "    return s.str.upper()\n",
    "\n",
    "upperUDF = spark.udf.register(\"upper_udf\", upper_udf_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_gender와 get_gender_count 함수를 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gender(spark, file_path):\n",
    "    return spark.read.option(\"header\", True).csv(file_path)\n",
    "\n",
    "def get_gender_count(spark, df, field_to_count):\n",
    "    df.createOrReplaceTempView(\"namegender_test\")\n",
    "    return spark.sql(f\"SELECT {field_to_count}, count(1) count FROM namegender_test GROUP BY 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     F|   65|\n",
      "|     M|   28|\n",
      "|Unisex|    7|\n",
      "+------+-----+\n",
      "\n",
      "+----------+\n",
      "|      NAME|\n",
      "+----------+\n",
      "|  ADALEIGH|\n",
      "|     AMRYN|\n",
      "|    APURVA|\n",
      "|    ARYION|\n",
      "|    ALIXIA|\n",
      "|ALYSSAROSE|\n",
      "|    ARVELL|\n",
      "|     AIBEL|\n",
      "|   ATIYYAH|\n",
      "|     ADLIE|\n",
      "|    ANYELY|\n",
      "|    AAMONI|\n",
      "|     AHMAN|\n",
      "|    ARLANE|\n",
      "|   ARMONEY|\n",
      "|   ATZHIRY|\n",
      "| ANTONETTE|\n",
      "|   AKEELAH|\n",
      "| ABDIKADIR|\n",
      "|    ARINZE|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = load_gender(spark, path+\"/name_gender.csv\")\n",
    "get_gender_count(spark, df, \"gender\").show()\n",
    "df.select(upperUDF(\"name\").alias(\"NAME\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(upperUDF(\"name\").alias(\"NAME\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유닛 테스트 코드 붙여보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Yeojun\\\\Documents\\\\GitHub\\\\Programmers_DevCourse\\\\학습내용\\\\빅데이터 처리 시스템, 하둡과 Spark\\\\data\\\\name.gender.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path + \"\\\\name.gender.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import TestCase\n",
    "\n",
    "# 일반적으로는 아래 함수가 정의된 모듈을 임포트하고 그걸 테스트\n",
    "# - upper_udf_f\n",
    "# - load_gender\n",
    "# - get_gender_count\n",
    "# Local Standalone Moder Spark으로 기능 테스트\n",
    "\n",
    "# 이외에도 2가지 방법이 더 존재\n",
    "# - from pyspark.sql.tests import SparkTestingBase\n",
    "# - pytest-spark (pytest testing framework plugin)\n",
    "\n",
    "class UtilsTestCase(TestCase):\n",
    "    spark = None\n",
    "    path = \"C:\\\\Users\\\\Yeojun\\\\Documents\\\\GitHub\\\\Programmers_DevCourse\\\\학습내용\\\\빅데이터 처리 시스템, 하둡과 Spark\\\\data\"\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls) -> None:\n",
    "        cls.spark = SparkSession.builder \\\n",
    "            .appName(\"Spark Unit Test\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "    def test_datafile_loading(self):\n",
    "        sample_df = load_gender(self.spark, path + \"\\\\name_gender.csv\")\n",
    "        result_count = sample_df.count()\n",
    "        self.assertEqual(result_count, 100, \"Record count should be 100\")\n",
    "\n",
    "    def test_gender_count(self):\n",
    "        sample_df = load_gender(self.spark, path + \"\\\\name_gender.csv\")\n",
    "        count_list = get_gender_count(self.spark, sample_df, \"gender\").collect()\n",
    "        count_dict = dict()\n",
    "        for row in count_list:\n",
    "            count_dict[row[\"gender\"]] = row['count']\n",
    "        self.assertEqual(count_dict[\"F\"], 65, \"Count for F should be 65\")\n",
    "        self.assertEqual(count_dict[\"M\"], 28, \"Count for M should be 28\")\n",
    "        self.assertEqual(count_dict[\"Unisex\"], 7, \"Count for Unisex should be 7\")\n",
    "\n",
    "    def test_upper_udf(self):\n",
    "        test_data = [\n",
    "            { \"name\": \"John Kim\" },\n",
    "            { \"name\": \"Johnny Kim\"},\n",
    "            { \"name\": \"1234\" }\n",
    "        ]\n",
    "        expected_results = [ \"JOHN KIM\", \"JOHNNY KIM\", \"1234\" ]\n",
    "\n",
    "        upperUDF = self.spark.udf.register(\"upper_udf\", upper_udf_f)\n",
    "        test_df = self.spark.createDataFrame(test_data)\n",
    "        names = test_df.select(\"name\", upperUDF(\"name\").alias(\"NAME\")).collect()\n",
    "        results = []\n",
    "        for name in names:\n",
    "            results.append(name[\"NAME\"])\n",
    "        self.assertCountEqual(results, expected_results)\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def tearDownClass(cls) -> None:\n",
    "        cls.spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_datafile_loading (__main__.UtilsTestCase) ... ok\n",
      "test_gender_count (__main__.UtilsTestCase) ... c:\\Users\\Yeojun\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:776: ResourceWarning: unclosed <socket.socket fd=4008, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 13786), raddr=('127.0.0.1', 13785)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "ok\n",
      "test_upper_udf (__main__.UtilsTestCase) ... c:\\Users\\Yeojun\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:776: ResourceWarning: unclosed <socket.socket fd=3760, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 13885), raddr=('127.0.0.1', 13884)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 4.849s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x239029828f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

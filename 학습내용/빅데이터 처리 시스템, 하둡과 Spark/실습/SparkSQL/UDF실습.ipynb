{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Session 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "## 드라이버 및 실행기 메모리 제한 \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Python Spark UDF\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.driver.memory\",\"2g\") \\\n",
    "    .config(\"spark.executor.memory\",\"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe/SQL에 UDF사용해보기 #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(1, \"john jones\"),\n",
    "        (2, \"tracey smith\"),\n",
    "        (3, \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|Name        |Curated Name|\n",
      "+-----+------------+------------+\n",
      "|1    |john jones  |JOHN JONES  |\n",
      "|2    |tracey smith|TRACEY SMITH|\n",
      "|3    |amy sanders |AMY SANDERS |\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "upperUDF = F.udf(lambda z:z.upper())\n",
    "\n",
    "df.withColumn(\"Curated Name\", upperUDF(\"Name\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_udf(s):\n",
    "    return s.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|Name        |Curated Name|\n",
      "+-----+------------+------------+\n",
      "|1    |john jones  |JOHN JONES  |\n",
      "|2    |tracey smith|TRACEY SMITH|\n",
      "|3    |amy sanders |AMY SANDERS |\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upperUDF = F.udf(upper_udf, StringType())\n",
    "\n",
    "df.withColumn(\"Curated Name\", upperUDF(\"Name\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|        Name|Curated Name|\n",
      "+------------+------------+\n",
      "|  john jones|  JOHN JONES|\n",
      "|tracey smith|TRACEY SMITH|\n",
      "| amy sanders| AMY SANDERS|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Name\", upperUDF(\"Name\").alias(\"Curated Name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def upper_udf_f(s:pd.Series)-> pd.Series:\n",
    "    return s.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|upper_udf(aBcD)|\n",
      "+---------------+\n",
      "|           ABCD|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upperUDF = spark.udf.register(\"upper_udf\",upper_udf_f)\n",
    "spark.sql(\"SELECT upper_udf('aBcD')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|        name|upper_udf(name)|\n",
      "+------------+---------------+\n",
      "|  john jones|     JOHN JONES|\n",
      "|tracey smith|   TRACEY SMITH|\n",
      "| amy sanders|    AMY SANDERS|\n",
      "+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\", upperUDF(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|        name|Curated Name|\n",
      "+------------+------------+\n",
      "|  john jones|  JOHN JONES|\n",
      "|tracey smith|TRACEY SMITH|\n",
      "| amy sanders| AMY SANDERS|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"test\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT name, upper_udf(name) `Curated Name` FROM test\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe/SQL에 UDF사용해보기 #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  5|  5| 10|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\"a\": 1, \"b\":2},\n",
    "    {\"a\": 5, \"b\":5}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.withColumn(\"c\", F.udf(lambda x, y : x+y)(\"a\" , \"b\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|sum|\n",
      "+---+\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def plus(x, y):\n",
    "    return x + y\n",
    "\n",
    "plusUDF = spark.udf.register(\"plus\",plus)\n",
    "spark.sql(\"SELECT plus(1, 2) sum\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  p|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  5|  5| 10|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"p\", plusUDF(\"a\",\"b\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|sum|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  5|  5| 10|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"test\")\n",
    "spark.sql(\"SELECT a, b, plus(a, b) sum FROM test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe에서 UDAF 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|average_udf(b)|\n",
      "+--------------+\n",
      "|           3.5|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(FloatType())\n",
    "def average_udf_f(v: pd.Series) -> float:\n",
    "    return v.mean()\n",
    "\n",
    "averageUDF = spark.udf.register('average_udf', average_udf_f)\n",
    "spark.sql('SELECT average_udf(b) FROM test').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|  3.5|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(averageUDF(\"b\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame에 explode 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+\n",
      "|      name|     knownLanguages|          properties|\n",
      "+----------+-------------------+--------------------+\n",
      "|     James|      [Java, Scala]|{eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java, NULL]|{eye -> NULL, hai...|\n",
      "|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n",
      "|Washington|               NULL|                NULL|\n",
      "| Jefferson|             [1, 2]|                  {}|\n",
      "+----------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- col: string (nullable = true)\n",
      "\n",
      "+---------+------+\n",
      "|     name|   col|\n",
      "+---------+------+\n",
      "|    James|  Java|\n",
      "|    James| Scala|\n",
      "|  Michael| Spark|\n",
      "|  Michael|  Java|\n",
      "|  Michael|  NULL|\n",
      "|   Robert|CSharp|\n",
      "|   Robert|      |\n",
      "|Jefferson|     1|\n",
      "|Jefferson|     2|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(df.name, explode(df.knownLanguages))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하나의 레코드에서 다수의 레코드를 만들어내는 예제(Order to 1+Items)\n",
    "데이터 경로지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark로 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "order = spark.read.options(delimiter='\\t').option(\"header\",\"true\").csv(path + \"/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- items: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"id\", StringType()),\n",
    "        StructField(\"quantity\", LongType())\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order.withColumn(\"item\", explode(from_json(\"items\", struct))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = order.withColumn(\"item\", explode(from_json(\"items\", struct))).drop(\"items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|    order_id|                item|\n",
      "+------------+--------------------+\n",
      "|860196503764|{DAILY SPF, 18837...|\n",
      "|860292645076|{DAILY SPF — Bund...|\n",
      "|860320956628|{DAILY SPF, 18839...|\n",
      "|860321513684|{DAILY SPF, 18839...|\n",
      "|862930665684|{DAILY SPF, 18879...|\n",
      "+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- item: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- quantity: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items.createOrReplaceTempView(\"order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|     order_id|avg_count|\n",
      "+-------------+---------+\n",
      "|1816674631892|      500|\n",
      "|1821860430036|      300|\n",
      "|2186043064532|      208|\n",
      "|2118824558804|      200|\n",
      "|2143034474708|      200|\n",
      "+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT order_id, CAST(average_udf(item.quantity) as decimal) avg_count\n",
    "FROM order_items\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|quantity|\n",
      "+--------+\n",
      "|     500|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT item.quantity FROM order_items WHERE order_id = '1816674631892'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='order_items', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='test', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "!=\n",
      "%\n",
      "&\n",
      "*\n",
      "+\n",
      "-\n",
      "/\n",
      "<\n",
      "<=\n",
      "<=>\n",
      "<>\n",
      "=\n",
      "==\n",
      ">\n",
      ">=\n",
      "^\n",
      "abs\n",
      "acos\n",
      "acosh\n",
      "add_months\n",
      "aes_decrypt\n",
      "aes_encrypt\n",
      "aggregate\n",
      "and\n",
      "any\n",
      "any_value\n",
      "approx_count_distinct\n",
      "approx_percentile\n",
      "array\n",
      "array_agg\n",
      "array_append\n",
      "array_compact\n",
      "array_contains\n",
      "array_distinct\n",
      "array_except\n",
      "array_insert\n",
      "array_intersect\n",
      "array_join\n",
      "array_max\n",
      "array_min\n",
      "array_position\n",
      "array_prepend\n",
      "array_remove\n",
      "array_repeat\n",
      "array_size\n",
      "array_sort\n",
      "array_union\n",
      "arrays_overlap\n",
      "arrays_zip\n",
      "ascii\n",
      "asin\n",
      "asinh\n",
      "assert_true\n",
      "atan\n",
      "atan2\n",
      "atanh\n",
      "avg\n",
      "base64\n",
      "between\n",
      "bigint\n",
      "bin\n",
      "binary\n",
      "bit_and\n",
      "bit_count\n",
      "bit_get\n",
      "bit_length\n",
      "bit_or\n",
      "bit_xor\n",
      "bitmap_bit_position\n",
      "bitmap_bucket_number\n",
      "bitmap_construct_agg\n",
      "bitmap_count\n",
      "bitmap_or_agg\n",
      "bool_and\n",
      "bool_or\n",
      "boolean\n",
      "bround\n",
      "btrim\n",
      "cardinality\n",
      "case\n",
      "cast\n",
      "cbrt\n",
      "ceil\n",
      "ceiling\n",
      "char\n",
      "char_length\n",
      "character_length\n",
      "chr\n",
      "coalesce\n",
      "collect_list\n",
      "collect_set\n",
      "concat\n",
      "concat_ws\n",
      "contains\n",
      "conv\n",
      "convert_timezone\n",
      "corr\n",
      "cos\n",
      "cosh\n",
      "cot\n",
      "count\n",
      "count_if\n",
      "count_min_sketch\n",
      "covar_pop\n",
      "covar_samp\n",
      "crc32\n",
      "csc\n",
      "cume_dist\n",
      "curdate\n",
      "current_catalog\n",
      "current_database\n",
      "current_date\n",
      "current_schema\n",
      "current_timestamp\n",
      "current_timezone\n",
      "current_user\n",
      "date\n",
      "date_add\n",
      "date_diff\n",
      "date_format\n",
      "date_from_unix_date\n",
      "date_part\n",
      "date_sub\n",
      "date_trunc\n",
      "dateadd\n",
      "datediff\n",
      "datepart\n",
      "day\n",
      "dayofmonth\n",
      "dayofweek\n",
      "dayofyear\n",
      "decimal\n",
      "decode\n",
      "degrees\n",
      "dense_rank\n",
      "div\n",
      "double\n",
      "e\n",
      "element_at\n",
      "elt\n",
      "encode\n",
      "endswith\n",
      "equal_null\n",
      "every\n",
      "exists\n",
      "exp\n",
      "explode\n",
      "explode_outer\n",
      "expm1\n",
      "extract\n",
      "factorial\n",
      "filter\n",
      "find_in_set\n",
      "first\n",
      "first_value\n",
      "flatten\n",
      "float\n",
      "floor\n",
      "forall\n",
      "format_number\n",
      "format_string\n",
      "from_csv\n",
      "from_json\n",
      "from_unixtime\n",
      "from_utc_timestamp\n",
      "get\n",
      "get_json_object\n",
      "getbit\n",
      "greatest\n",
      "grouping\n",
      "grouping_id\n",
      "hash\n",
      "hex\n",
      "histogram_numeric\n",
      "hll_sketch_agg\n",
      "hll_sketch_estimate\n",
      "hll_union\n",
      "hll_union_agg\n",
      "hour\n",
      "hypot\n",
      "if\n",
      "ifnull\n",
      "ilike\n",
      "in\n",
      "initcap\n",
      "inline\n",
      "inline_outer\n",
      "input_file_block_length\n",
      "input_file_block_start\n",
      "input_file_name\n",
      "instr\n",
      "int\n",
      "isnan\n",
      "isnotnull\n",
      "isnull\n",
      "java_method\n",
      "json_array_length\n",
      "json_object_keys\n",
      "json_tuple\n",
      "kurtosis\n",
      "lag\n",
      "last\n",
      "last_day\n",
      "last_value\n",
      "lcase\n",
      "lead\n",
      "least\n",
      "left\n",
      "len\n",
      "length\n",
      "levenshtein\n",
      "like\n",
      "ln\n",
      "localtimestamp\n",
      "locate\n",
      "log\n",
      "log10\n",
      "log1p\n",
      "log2\n",
      "lower\n",
      "lpad\n",
      "ltrim\n",
      "luhn_check\n",
      "make_date\n",
      "make_dt_interval\n",
      "make_interval\n",
      "make_timestamp\n",
      "make_timestamp_ltz\n",
      "make_timestamp_ntz\n",
      "make_ym_interval\n",
      "map\n",
      "map_concat\n",
      "map_contains_key\n",
      "map_entries\n",
      "map_filter\n",
      "map_from_arrays\n",
      "map_from_entries\n",
      "map_keys\n",
      "map_values\n",
      "map_zip_with\n",
      "mask\n",
      "max\n",
      "max_by\n",
      "md5\n",
      "mean\n",
      "median\n",
      "min\n",
      "min_by\n",
      "minute\n",
      "mod\n",
      "mode\n",
      "monotonically_increasing_id\n",
      "month\n",
      "months_between\n",
      "named_struct\n",
      "nanvl\n",
      "negative\n",
      "next_day\n",
      "not\n",
      "now\n",
      "nth_value\n",
      "ntile\n",
      "nullif\n",
      "nvl\n",
      "nvl2\n",
      "octet_length\n",
      "or\n",
      "overlay\n",
      "parse_url\n",
      "percent_rank\n",
      "percentile\n",
      "percentile_approx\n",
      "pi\n",
      "pmod\n",
      "posexplode\n",
      "posexplode_outer\n",
      "position\n",
      "positive\n",
      "pow\n",
      "power\n",
      "printf\n",
      "quarter\n",
      "radians\n",
      "raise_error\n",
      "rand\n",
      "randn\n",
      "random\n",
      "range\n",
      "rank\n",
      "reduce\n",
      "reflect\n",
      "regexp\n",
      "regexp_count\n",
      "regexp_extract\n",
      "regexp_extract_all\n",
      "regexp_instr\n",
      "regexp_like\n",
      "regexp_replace\n",
      "regexp_substr\n",
      "regr_avgx\n",
      "regr_avgy\n",
      "regr_count\n",
      "regr_intercept\n",
      "regr_r2\n",
      "regr_slope\n",
      "regr_sxx\n",
      "regr_sxy\n",
      "regr_syy\n",
      "repeat\n",
      "replace\n",
      "reverse\n",
      "right\n",
      "rint\n",
      "rlike\n",
      "round\n",
      "row_number\n",
      "rpad\n",
      "rtrim\n",
      "schema_of_csv\n",
      "schema_of_json\n",
      "sec\n",
      "second\n",
      "sentences\n",
      "sequence\n",
      "session_window\n",
      "sha\n",
      "sha1\n",
      "sha2\n",
      "shiftleft\n",
      "shiftright\n",
      "shiftrightunsigned\n",
      "shuffle\n",
      "sign\n",
      "signum\n",
      "sin\n",
      "sinh\n",
      "size\n",
      "skewness\n",
      "slice\n",
      "smallint\n",
      "some\n",
      "sort_array\n",
      "soundex\n",
      "space\n",
      "spark_partition_id\n",
      "split\n",
      "split_part\n",
      "sql_keywords\n",
      "sqrt\n",
      "stack\n",
      "startswith\n",
      "std\n",
      "stddev\n",
      "stddev_pop\n",
      "stddev_samp\n",
      "str_to_map\n",
      "string\n",
      "struct\n",
      "substr\n",
      "substring\n",
      "substring_index\n",
      "sum\n",
      "tan\n",
      "tanh\n",
      "timestamp\n",
      "timestamp_micros\n",
      "timestamp_millis\n",
      "timestamp_seconds\n",
      "tinyint\n",
      "to_binary\n",
      "to_char\n",
      "to_csv\n",
      "to_date\n",
      "to_json\n",
      "to_number\n",
      "to_timestamp\n",
      "to_timestamp_ltz\n",
      "to_timestamp_ntz\n",
      "to_unix_timestamp\n",
      "to_utc_timestamp\n",
      "to_varchar\n",
      "transform\n",
      "transform_keys\n",
      "transform_values\n",
      "translate\n",
      "trim\n",
      "trunc\n",
      "try_add\n",
      "try_aes_decrypt\n",
      "try_avg\n",
      "try_divide\n",
      "try_element_at\n",
      "try_multiply\n",
      "try_subtract\n",
      "try_sum\n",
      "try_to_binary\n",
      "try_to_number\n",
      "try_to_timestamp\n",
      "typeof\n",
      "ucase\n",
      "unbase64\n",
      "unhex\n",
      "unix_date\n",
      "unix_micros\n",
      "unix_millis\n",
      "unix_seconds\n",
      "unix_timestamp\n",
      "upper\n",
      "url_decode\n",
      "url_encode\n",
      "user\n",
      "uuid\n",
      "var_pop\n",
      "var_samp\n",
      "variance\n",
      "version\n",
      "weekday\n",
      "weekofyear\n",
      "when\n",
      "width_bucket\n",
      "window\n",
      "window_time\n",
      "xpath\n",
      "xpath_boolean\n",
      "xpath_double\n",
      "xpath_float\n",
      "xpath_int\n",
      "xpath_long\n",
      "xpath_number\n",
      "xpath_short\n",
      "xpath_string\n",
      "xxhash64\n",
      "year\n",
      "zip_with\n",
      "|\n",
      "||\n",
      "~\n",
      "average_udf\n",
      "plus\n",
      "upper_udf\n"
     ]
    }
   ],
   "source": [
    "for f in spark.catalog.listFunctions():\n",
    "    print(f[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

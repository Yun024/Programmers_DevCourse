{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Python Spark Windowing\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.driver.memory\",\"2g\") \\\n",
    "    .config(\"spark.executor.memory\",\"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_test = [\n",
    "    { 'value': 1, 'name': 'Luka' },\n",
    "    { 'value': 2, 'name': 'Luka'},\n",
    "    { 'value': 3, 'name': 'Dirk' },\n",
    "    { 'value': 4, 'name': 'Dirk' },\n",
    "    { 'value': 5, 'name': 'Luka' },\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(rows_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"rows_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|value|rolling_sum|\n",
      "+-----+-----------+\n",
      "|    1|          6|\n",
      "|    2|         10|\n",
      "|    3|         15|\n",
      "|    4|         14|\n",
      "|    5|         12|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    value,\n",
    "    SUM(value) OVER ( \n",
    "        order by value \n",
    "        rows between 2 preceding and 2 following\n",
    "    ) AS rolling_sum\n",
    "FROM rows_test\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|value|rolling_sum|\n",
      "+-----+-----------+\n",
      "|    1|          6|\n",
      "|    2|         10|\n",
      "|    3|         15|\n",
      "|    4|         15|\n",
      "|    5|         15|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    value,\n",
    "    SUM(value) OVER (\n",
    "        order by value \n",
    "        rows between unbounded preceding and 2 following\n",
    "    ) AS rolling_sum\n",
    "FROM rows_test\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------+---------+\n",
      "|name|value|min_value|max_value|\n",
      "+----+-----+---------+---------+\n",
      "|Dirk|    3|        3|        4|\n",
      "|Dirk|    4|        3|        4|\n",
      "|Luka|    1|        1|        5|\n",
      "|Luka|    2|        1|        5|\n",
      "|Luka|    5|        1|        5|\n",
      "+----+-----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "*,\n",
    "FIRST_VALUE(value) OVER (\n",
    "    partition by name\n",
    "    order by value \n",
    "    rows between unbounded preceding and unbounded following\n",
    ") AS min_value,\n",
    "LAST_VALUE(value) OVER (\n",
    "    partition by name\n",
    "    order by value \n",
    "    rows between unbounded preceding and unbounded following\n",
    ") AS max_value    \n",
    "FROM rows_test\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowflae 연결 정보\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"C:/Users/Yeojun/airflow/airflow-dag/.env\")\n",
    "\n",
    "user = os.environ.get(\"SNOWFLAKE_USER\")\n",
    "password = os.environ.get(\"SNOWFLAKE_PASS\")\n",
    "account = os.environ.get(\"SNOWFLAKE_ACCOUNT\")\n",
    "jdbc_url = f\"jdbc:snowflake://{account}.snowflakecomputing.com/?warehouse=COMPUTE_WH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_session_channel = spark.read \\\n",
    ".format(\"jdbc\")\\\n",
    ".option(\"url\",jdbc_url)\\\n",
    ".option(\"dbtable\",\"dev.raw_data.user_session_channel\")\\\n",
    ".option(\"user\",user)\\\n",
    ".option(\"password\",password)\\\n",
    ".option(\"driver\", \"net.snowflake.client.jdbc.SnowflakeDriver\") \\\n",
    ".load()\n",
    "\n",
    "df_session_timestamp = spark.read \\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\",jdbc_url)\\\n",
    "    .option(\"dbtable\",\"dev.raw_data.session_timestamp\")\\\n",
    "    .option(\"user\",user)\\\n",
    "    .option(\"password\",password)\\\n",
    "    .option(\"driver\", \"net.snowflake.client.jdbc.SnowflakeDriver\") \\\n",
    "    .load()\n",
    "\n",
    "df_session_transaction = spark.read \\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\",jdbc_url)\\\n",
    "    .option(\"dbtable\",\"dev.raw_data.session_transaction\")\\\n",
    "    .option(\"user\",user)\\\n",
    "    .option(\"password\",password)\\\n",
    "    .option(\"driver\", \"net.snowflake.client.jdbc.SnowflakeDriver\") \\\n",
    "    .load()\n",
    "\n",
    "df_user_session_channel.createOrReplaceTempView(\"user_session_channel\")\n",
    "df_session_timestamp.createOrReplaceTempView(\"session_timestamp\")\n",
    "df_session_transaction.createOrReplaceTempView(\"session_transaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+\n",
      "|userid|first_channel|last_channel|\n",
      "+------+-------------+------------+\n",
      "|    27|      Youtube|   Instagram|\n",
      "|    29|        Naver|       Naver|\n",
      "|    33|       Google|     Youtube|\n",
      "|    34|      Youtube|       Naver|\n",
      "|    36|        Naver|     Youtube|\n",
      "|    40|      Youtube|      Google|\n",
      "|    41|     Facebook|     Youtube|\n",
      "|    44|        Naver|   Instagram|\n",
      "|    45|      Youtube|   Instagram|\n",
      "|    59|    Instagram|   Instagram|\n",
      "+------+-------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_last_channel_df = spark.sql(\"\"\"\n",
    "WITH RECORD AS (\n",
    "  SELECT /*사용자의 유입에 따른, 채널 순서 매기는 쿼리*/\n",
    "      userid,\n",
    "      channel, \n",
    "      ROW_NUMBER() OVER (PARTITION BY userid ORDER BY ts ASC) AS seq_first,\n",
    "      ROW_NUMBER() OVER (PARTITION BY userid ORDER BY ts DESC) AS seq_last\n",
    "  FROM user_session_channel u\n",
    "  LEFT JOIN session_timestamp t\n",
    "    ON u.sessionid = t.sessionid\n",
    ")\n",
    "SELECT /*유저의 첫번째 유입채널, 마지막 유입 채널 구하기*/\n",
    "      f.userid,\n",
    "      f.channel first_channel,\n",
    "      l.channel last_channel\n",
    "FROM RECORD f\n",
    "INNER JOIN RECORD l ON f.userid = l.userid\n",
    "WHERE f.seq_first = 1 and l.seq_last = 1\n",
    "ORDER BY userid\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+\n",
      "|userid|First_Channel|Last_Channel|\n",
      "+------+-------------+------------+\n",
      "|    27|      Youtube|   Instagram|\n",
      "|    29|        Naver|       Naver|\n",
      "|    33|       Google|     Youtube|\n",
      "|    34|      Youtube|       Naver|\n",
      "|    36|        Naver|     Youtube|\n",
      "|    40|      Youtube|      Google|\n",
      "|    41|     Facebook|     Youtube|\n",
      "|    44|        Naver|   Instagram|\n",
      "|    45|      Youtube|   Instagram|\n",
      "|    59|    Instagram|   Instagram|\n",
      "+------+-------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_last_channel_df2 = spark.sql(\"\"\"\n",
    "SELECT DISTINCT A.userid,\n",
    "    FIRST_VALUE(A.channel) over(partition by A.userid order by B.ts\n",
    "rows between unbounded preceding and unbounded following) AS First_Channel,\n",
    "    LAST_VALUE(A.channel) over(partition by A.userid order by B.ts\n",
    "rows between unbounded preceding and unbounded following) AS Last_Channel\n",
    "FROM user_session_channel A\n",
    "LEFT JOIN session_timestamp B\n",
    "ON A.sessionid = B.sessionid\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# Redshift 소개
## Redshift 특징 소개
### Redshift의 특징
- AWS에서 지원하는 데이터 웨어하우스 서비스
- 2 PB의 데이터까지 처리 가능
    + 최소 160GB로 시작해서 점진적으로 용량 증감 가능
- Still OLAP
    + 응답속도가 빠르지 않기 때문에 프로덕션 데이터베이스로 사용불가
- 컬럼 기반 스토리지
    + 레코드 별로 저장하는 것이 아니라 컬럼별로 저장함
    + 컬럼별 압축이 가능하며 컬럼을 추가하거나 삭제하는 것이 아주 빠름
- 벌크 업데이트 지원
    + 레코드가 들어있는 파일을 S3로 복사 후 COPY 커맨드로 Redshift로 일괄 복사
- 고정 용량/비용 SQL 엔진
    + 최근 가변 비용 옵션도 제공 (Redshift Serverless)
- 데이터 공유 기능(Datashare)
    + 다른 AWS 계정과 특정 데이터 공유 가능
    + Snowflake의 기능과 따라함
- 다른 데이터 웨어하우스처럼 Primary key uniqueness를 보장하지 않음
    + 프로덕션 데이터베이스는 Primary key uniqueness를 보장함

### Redshift는 SQL 기반 관계형 데이터베이스
- Postgresq 8.x와 SQL이 호환됨
    + 하지만 Postgresql 8.x의 모든 기능을 지원하지는 않음
    + 예를 들어 text타입이 존재하지 않음
- Postgresql 8.x를 지원하는 툴이나 라이브러리로 액세스 가능
    + JDBC/ODBC
- 다시 한번 SQL이 메인 언어라는 점 명시
    + 그래서 데이터 모델링(테이블 디자인)이 아주 중요

### Redshift의 스케일링 방식
- 용량이 부족해질 때마다 새로운 노드를 추가하는 방식으로 스케일링
    + Scale Out방식과 Scale Up방식
    + dc2.large가 하나면 최대 0.16TB까지의 용량을 갖게됨
    + 공간이 부족해지면
        - dc2.large 한대를 더 추가 -> 총 0.32TB (Scale Out)
        - 아니면 사양을 더 좋은 것으로 업그레이드 -> dc2.8xlarge 한대로 교체 (Scale Up)
- 이를 Resizing이라 부르며 Auto Scaling 옵션을 설정하면 자동으로 이뤄짐
- 이는 Snowflake나 BigQuery의 방식과는 굉장히 다름
    + 여기서는 특별히 용량이 정해져있지 않고 쿼리를 처리하기 위해 사용한 리소스에 해당하는 비용 지불
        - 즉 Snowflake와 BigQuery가 훨씬 더 스케일하는 데이터베이스 기술이라 볼 수 있음
        - 장단점 존재 -> 비용의 예측이 불가능하다는 단점 존재
- Redshift에도 가변비용 옵션 존재 `Redshift Serverless`
    + 뒤에 데모에서 사용해볼 예정 (Pay as You Go)

### Redshift 최적화는 굉장히 복잡
- Redshift가 두 대 이상의 노드로 구성되면 한 테이블의 레코드 저장방식은?
    + 분산 저장되어야 함
    + Data Skewed(스큐) 조심해야 함
    + 또 한 노드 내에서는 순서를 정해주어야 함

### Redshift의 레코드 분배와 저장 방식
- Redshift가 두 대 이상의 노드로 구성되면 그 시점부터 테이블 최적화가 중요
    + 한 테이블의 레코드들을 어떻게 다수의 노드로 분배할 것이냐!
- Distkey, Diststyle, Sortkey 세 개의 키워드를 알아야함
    + Diststyle은 레코드 분배가 어떻게 이뤄지는지를 결정
        - all, even, key (디폴트는 `even`)
    + Distkey는 레코드가 어떤 컬럼을 기준으로 배포되는지 나타냄(diststyle이 key인 경우)
    + Sortkey는 레코드가 한 노드내에서 어떤 컬럼을 기준으로 정렬되는지 나타냄
        - 이는 보통 타임스탬프 필드가 됨
- Diststyle이 key인 경우 컬럼 선택이 잘못되면?
    + 레코드 분포에 Skew가 발생 -> 분산처리의 효율성이 사라짐
    + BigQuery나 Snowflake에서는 이런 속성을 개발자가 지정할 필요가 없음(시스템이 알아서 선택)

### Redshift의 레코드 분배와 저장 방식 예
- my_table의 레코드들은 column1의 값을 기준으로 분배됨
- 같은 노드(슬라이스)안에서는 column3의 값을 기준으로 소팅이 됨
- SORTKYE는 TIMESTAMP 컬럼을 사용하는 것이 일반적
```SQL
CREATE TABLE my_table(
    column1 INT,
    column2 VARCHAR(50),
    column3 TIMESTAMP,
    column4 DECIMAL(18,2)
)DISTSTYLE KEY DISTKEY(column1) SORTKEY(column3);
```

### Redshift의 벌크 업데이트 방식 - COPY SQL
1. 소스로부터 데이터 추출
2. S3에 업로드(보통 Parquet 포맷을 선호)
3. COPY SQL로 S3에서 Redshift 테이블로 한번에 복사

### Redshift의 기본 데이터 타입
- `SMALLINT(INT2)` `INTEGER(INT, INT4)` `BIGINT(INT8)`
- `DECIMAL(NUMERIC)` `REAL(FLOAT4)` `DOUBLE PRECISION(FLOAT8)`
- `BOOLEAN(BOOL)` 
- `CHAR(CHARACTER)` `VARCHAR(CHARACTER VARYING)` `TEXT(VARCHAR(256))`
- `DATE` `TIMESTAMP`

### Redshift의 고급 데이터 타입
- GEOMETRY
- GEOGRAPHY
- HLLSKETCH
- SUPER

## Redshift 설치(Trial)
### Redshift 설치 과정 데모
- 먼저 본인 AWS 계정 만들고 로그인
- Redshift Serverless를 선택하고 Free Trial인 것 확인하고 진행
    + Free Trial이 만기되기 전에 꼭 셧다운하고 주어진 비용도 최대 300$이니 반드시 확인

※ Snapshot `일종의 백업`

### Google colab을 통해 접속하기
- Redshift의 endpoint, host, port, db_name
    + 작업 그룹 `endpoint` 확인
- Access할 Account 세팅
    + admin세팅
    + AWS의 IAM기능을 통해 별도의 Account 세팅
    + 네임스페이스에서 `UserId` `Password` 확인
- %sql postgresql://[userid]:[password]@[endpoint] `에러발생`
    + endpoint 퍼블릭 액세스 가능하도록 재설정
        - redshift serverless work group에서 체크박스 체크
    + Network and security의 VPC security group 클릭
        - 인바운드 규칙 클릭
        - 포트범위 `5439` 소스 `0.0.0.0/0` 

## Redshift 초기 설정(스키마, 그룹, 유저)
### Redshift Schema
다른 기타 관계형 데이터베이스와 동일한 구조
```
DEV ⭢ raw_data : ETL 결과가 들어감
    ↳ analytics : ELT 결과가 들어감
    ↳ adhoc     : 테스트용 테이블이 들어감
    ↳ pii       : 개인정보가 들어감
```

### 스키마(Schema) 설정
```sql
-- 모든 스키마 조회하기
select * from pg_namespace;
CREATE Schema [스키마명] ;
```

### 사용자(User)생성
```sql
-- 모든 사용자 리스트하기
select * from pg_user;
CREATE USER [사용자명] PASSWORD [패스워드];
```

### 그룹(Group)생성/설정
- 한 사용자는 다수의 그룹에 속할 수 있음
- 그룹의 문제는 계승이 안된다는 점
    + 너무 많은 그룹을 만들게 되면 관리가 힘들어짐
- 다음과 같은 그룹이 존재할 수 있음
    + admin을 위한 pii_users
    + 데이터 분석가를 위한 analytics_authors
    + 데이터 활용을 하는 개인을 위한 analytics_users

```sql
-- 모든 그룹을 리스트하기
select * from pg_group;
CREATE GROUP [그룹명];
ALTER GROUP [그룹명] ADD USER [사용자명];
```

### 역할(Role) 생성/설정
- 그룹과 달리 계승 구조를 만들 수 있음
- 사용자에게 부여될 수도 있고 다른 역할에 부여될 수도 있음
- 한 사용자는 다수의 역할에 소속 가능함

```sql
-- 모든 역할을 리스트하기
select * from SVV_ROLES;
CREATE ROLE [역할명];
GRANT ROLE [역할명] TO [사용자명 or 역할명];
```


## Redshift COPY 명령으로 테이블에 레코드 적재하기
### COPY와 AWS IAM 학습
- COPY 명령을 사용해 raw_data 스키마 밑 3개의 테이블에 레코드 적재하기
- 각 테이블을 CREATE TABLE 명령으로 raw_data 스키마 밑에 생성
- 각 테이블의 입력이 되는 CSV 파일을 먼저 S3로 복사하기 (벌크 업데이트)
    + S3 버킷부터 미리 생성(S3 웹콘솔)
- S3에서 해당 테이블로 복사를 하려면 Redshift가 S3 접근권한을 가져야함
    + Redshift가 S3에 접근할 수 있는 역할 만들기(IAM 웹콘솔)
    + 만들어진 역할을 Redshift 클러스터에 저장(Redshift 웹콘솔)

### raw_data 테스트 테이블 만들기 - 테이블 생성
- raw_data 스키마에 아래 세 테이블 만들기
    + 보통 이런 테이블들은 ETL을 통해 데이터 소스에서 복사해오는 형태로 이뤄짐

```sql
CREATE TABLE raw_data.user_session_channel(
    userid integer,
    sessionid varchar(32) primary key,
    channel varchar(32)
)
```

### raw_data 테스트 테이블 만들기 - S3 버킷 생성과 파일 업로드
- Redshift의 COPY SQL을 사용해서 앞서 3개의 테이블 내용을 적재해보자
- 먼저 입력이 되는 CSV 파일들을 적당한 위치에 다운로드 받기
    + user_session_channel.csv
    + session_timestamp.csv
    + session_transaction.csv
- AWS 콘솔에서 S3 bucket 하나 만들고 거기로 업로드하기
    + raw_data.user_session_channel
    + raw_data.session_timestamp
    + raw_data.session_transaction

### Redshift에 S3 접근 권한 설정
- Redshift가 앞서 만든 S3 버킷을 접근할 수 있어야함
- AWS IAM(Identity and Access Management)을 이용
    + 해당하는 역할(Role)을 만들고 Redshift에 부여해야 함

### Redshift의 S3 접근 권한용 IAM Role 만들기
1. 웹 콘솔에서 IAM 방문
2. 왼쪽 메뉴에서 Roles 선택
3. Create Role을 선택하고 S3 접근 권한을 지정한 Role 생성
4. AWS service 선택 -> Redshift 선택
5. Redshift - Customizable 선택 -> Next Permissions 버튼 클릭
6. Filter Policies 박스에서 AmazonS3FullAccess를 찾음
    - 왼쪽의 체크박스를 선택하고 Next:Tags 버튼을 클릭
7. Next:Review 버튼을 클릭하고 최종 이름으로 redshift.read.s3을 지정하고 최종 생성

### redshift.read.s3 역할을 Redshift 클러스터에 지정
- Redshift 콘솔로 돌아가 해당 클러스의 Default Namespace를 선택
    + `Security and encryption` 탭 아래 `Manage IAM roles`라는 버튼 선택
- Manage IAM roles 박스에서 `Associate IAM roles` 메뉴를 선택
    + 앞서 만든 redshift.read.s3 권한을 지정하고 `Associate IAM roles` 버튼을 클릭

### COPY 명령을 사용해 앞서 CSV 파일들을 테이블로 복사(1)
- 앞서 생성한 테이블로 앞서 S3로 로딩한 파일을 벌크 업데이트 수행
- 이를 위해 COPY SQL 커맨드 사용
- [COPY SQL 문법](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html)
- CSV 파일이기에 delimiter로는 콤마(,)를 지정
- CSV 파일에서 문자열이 따옴표로 둘러싸인 경우 제거하기 위해 removequotes 지정
- CSV 파일의 첫번째 라인(헤더)을 무시하기 위해 `IGNOREHEADER 1`을 지정
- credentials에 앞서 Redshift에 지정한 Role을 사용
    + 해당 Role의 ARN을 읽어와야 함
- 만일 COPY 명령 실행 중에 에러가 나면 stl_load_errors 테이블의 내용을 보고 확인

### analytics 테스트 테이블 만들기
- analytics 스키마에 새로운 테이블 만들기
    + raw_data에 있는 테이블을 조인해서 새로 만들기(ELT)
    + 간단하게는 CTAS로 가능

### S3 bucket 생성
- Redshift와 같은 리전에서 버킷 생성
- 만들어진 버킷에서 폴더 만들기
- 폴더 안에서 업로드 진행
- 업로드한 파일이 S3 URL 복사
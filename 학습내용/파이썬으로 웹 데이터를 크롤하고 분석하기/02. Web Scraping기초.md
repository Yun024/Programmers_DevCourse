# Web Scraping 기초

# 인터넷 사용자간의 약속,HTTP

## 컴퓨터 한 대에서 Web까지의 여정
- 한 대의 컴퓨터에서 두 대의 컴퓨터를 연결하기 위한 네트워크(Network) 탄생!
- 이 네트워크를 묶어 근거리 지역 네트워크(Local Area Network, LAN) 탄생! 
- 범지구적으로 연결된 네트워크 Inter Network – 인터넷(Internet) 탄생!
- 인터넷에서 정보를 교환할 수 있는 환경 - WWW(World Wide Web, 줄여서 Web) 탄생!

## Web 상에서는 정보를 어떻게 주고받을까?
웹에서 정보 주고받기
- 클라이언트(Client): 정보를 요청하는 컴퓨터
- 서버(Server): 정보를 제공하는 컴퓨터
1. 클라이언트가 서버에게 정보를 요청
2. 요청에 대해서 서버가 작업을 수행
3. 수행한 작업의 결과를 클라이언트에게 응답

## HTTP(Hypertext Transfer Protocol) 
- 웹 상에서 정보를 주고받기 위한 약속
- HTTP요청(Request): 클라이언트에서 서버로 정보를 요청하는 것
- HTTP응답(Response): 요청된 정보에 대해 서버가 클라이언트에게 응답하는 것

## 정확한 정보를 요청하기 위해 필요한 정보
`클라이언트 -> 서버`
Host, Resource, Method
- 요청/응답에 대한 정보를 담는 Head: 보내는 사람, 받는 사람
- 내용물을 담는 Body
- Head: Method, path 등을 확인할 수 있음

## 정보의 손실 없이 응답하기 위해 필요한 정보
`클라이언트 <- 서버`
- Body가 담겨져 와야 하는데 상태 코드(404, 503)가 올 수도 있음
- Head – content-type, date,…을 확인할 수 있음


# 웹페이지와 HTML
## 웹 사이트와 웹 페이지
- 웹 페이지: 웹 속에 있는 문서 하나
- 웹 사이트: 웹 페이지의 모음 

## 웹 페이지는 어떻게 만들까요?
- 엄청나게 복잡한 html로 이루어짐
- 웹 브라우저의 역할: HTML 요청을 보내고, HTTP 응답에 담긴 HTML 문서를 우리가 보기 쉬운 형태로 **화면에 그려주는** 역할을 담당
- 웹 페이지는 HTML 이라는 형식으로 되어있음
- 웹 브라우저는 우리의 HTTP 요청을 보내고, 응답받은 HTML코드를 렌더링 해줌

## HTML의 구조
### HTML(HyperText Markup Language)
`<!DOCTYPE html>` : HTML5임을 명시
- 가장 바깥에 `<html>`태그로 감싸져있다.
- 여는태그:`<...>`, 닫는태그:`</...>`
- 크게 Head와 Body로 나눌 수 있음
- Head는 문서에 대한 정보(제목,언어 등)을 작성한다.
- Body는 문서의 내용(글,이미지,동영상 등)을 작성한다.

### HTML의 여러 특징
- 여러 태그(Tag)로 감싼 요소(Element)의 집합으로 이루어져 있음
- 태그로 내용을 묶어 글의 형식을 지정
- 태그는 그에 맞는 속성(attribute)를 가지기도 한다.
- 웹 브라우저마다 지원하는 태그와 속성이 다르다.

# 윤리적으로 웹 스크래핑/ 크롤링 진행하기

## 웹 스크래핑
- 특정한 목적으로 특정 웹 페이지에서 데이터를 추출하는 것 - 데이터 추출
    + 날씨 데이터 가져오기, 주식 데이터 가져오기

## 웹 크롤링
- URL을 타고다니며 반복적으로 데이터를 가져오는 과정 -  데이터 색인
    + 검색 엔진의 웹 크롤러

## 올바르게 HTTP 요청하기 위해 고려해야 할 것
- 웹 스크래핑/ 크롤링을 통해 어떤 목적을 달성하고자 하는가?
- 나의 웹 스크래핑/크롤링이 서버에 영향을 미치지 않는가?

## 웹 스크래핑을 할 때의 원칙
- 요청하고자 하는 서버에 과도한 부하를 주지 않는다.
- 가져온 정보를 사용할 때 저작권과 데이터베이스권에 위배되지 않는지 주의한다.  

## REP(Robot Exclusion Protocol) 로봇 배제 프로토콜
### robots.txt?
**robots.txt는 웹 사이트 및 웹 페이지를 수집하는 로봇들의 무단 접근을 방지하기 위해 만들어진 로봇 배제 표준(robots exclusion standard)이자 국제 권고안입니다.**

- 'User-agent' : 규칙이 적용되는 대상 사용자 에이전트
- 'Disallow' : 크롤링을 금지할 웹 페이지
- 'Allow' : 크롤링을 허용할 웹 페이지

※ 자세한 규약은 robots.txt [공식 홈페이지]("www.robotstxt.org")를 참조해주세요.

나의 User Agent 확인해보기: `https://www.whatismybrowser.com/detect/what-is-my-user-agent/`

```
robots.txt 예시
User-agent: *
Allow: / 
모든 user-agent에 대해서 접근을 허가

User-agent: *
DisAllow: / 
모든 user-agent에 대해서 접근을 거부

User-agent: MussgBot
DisAllow: / 
특정 user-agent에 대해서 접근을 거부
```


